{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1382bf3",
   "metadata": {
    "papermill": {
     "duration": 0.003812,
     "end_time": "2024-11-27T17:57:00.285579",
     "exception": false,
     "start_time": "2024-11-27T17:57:00.281767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Recommender Systems\n",
    "This project focuses on building a recommender system that predicts user ratings for books based on their past interactions. By understanding user preferences, the system aims to suggest books that align with individual tastes, enhancing the reading experience through personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1454936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:57:00.293698Z",
     "iopub.status.busy": "2024-11-27T17:57:00.293308Z",
     "iopub.status.idle": "2024-11-27T17:57:01.108713Z",
     "shell.execute_reply": "2024-11-27T17:57:01.107481Z"
    },
    "papermill": {
     "duration": 0.822532,
     "end_time": "2024-11-27T17:57:01.111316",
     "exception": false,
     "start_time": "2024-11-27T17:57:00.288784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dis-project-2-recommender-systems-f2024/sample_submission.csv\n",
      "/kaggle/input/dis-project-2-recommender-systems-f2024/books.csv\n",
      "/kaggle/input/dis-project-2-recommender-systems-f2024/train.csv\n",
      "/kaggle/input/dis-project-2-recommender-systems-f2024/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023226e6",
   "metadata": {
    "papermill": {
     "duration": 0.003447,
     "end_time": "2024-11-27T17:57:01.119407",
     "exception": false,
     "start_time": "2024-11-27T17:57:01.115960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb867c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:57:01.127982Z",
     "iopub.status.busy": "2024-11-27T17:57:01.127468Z",
     "iopub.status.idle": "2024-11-27T17:57:02.401665Z",
     "shell.execute_reply": "2024-11-27T17:57:02.400419Z"
    },
    "papermill": {
     "duration": 1.281161,
     "end_time": "2024-11-27T17:57:02.404106",
     "exception": false,
     "start_time": "2024-11-27T17:57:01.122945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and validation sets\n",
    "from tqdm import tqdm  # For displaying progress bars during iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e842035",
   "metadata": {
    "papermill": {
     "duration": 0.002858,
     "end_time": "2024-11-27T17:57:02.410229",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.407371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before we start, we need to load the two files needed for the project:\n",
    "- `train.csv`: contains the user ratings for the books.\n",
    "- `test.csv`: contains the user-book pairs for which we need to predict a rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e6b95c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:57:02.418970Z",
     "iopub.status.busy": "2024-11-27T17:57:02.417747Z",
     "iopub.status.idle": "2024-11-27T17:57:02.503919Z",
     "shell.execute_reply": "2024-11-27T17:57:02.502821Z"
    },
    "papermill": {
     "duration": 0.093067,
     "end_time": "2024-11-27T17:57:02.506469",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.413402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training, test, and books data\n",
    "train_df = pd.read_csv('/kaggle/input/dis-project-2-recommender-systems-f2024/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/dis-project-2-recommender-systems-f2024/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe93e1",
   "metadata": {
    "papermill": {
     "duration": 0.002949,
     "end_time": "2024-11-27T17:57:02.512749",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.509800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is devoted to the implementation of matrix factorization with bias (SVD-like approach) method to predict the rating a user might give to a book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937e831",
   "metadata": {
    "papermill": {
     "duration": 0.002927,
     "end_time": "2024-11-27T17:57:02.519081",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.516154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the first step, we define a function to compute the Root Mean Squared Error (RMSE), a key metric for evaluating the accuracy of our model's predictions. The RMSE calculates the average squared difference between the predicted and actual values, followed by taking the square root. We will use this function to evaluate the model's performance during hyperparameter tuning and on the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0956817c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:57:02.526975Z",
     "iopub.status.busy": "2024-11-27T17:57:02.526571Z",
     "iopub.status.idle": "2024-11-27T17:57:02.531615Z",
     "shell.execute_reply": "2024-11-27T17:57:02.530631Z"
    },
    "papermill": {
     "duration": 0.011407,
     "end_time": "2024-11-27T17:57:02.533644",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.522237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for the comutation of RMSE \n",
    "def compute_rmse(actual, predicted):\n",
    "    return np.sqrt(np.mean((actual - predicted) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423135df",
   "metadata": {
    "papermill": {
     "duration": 0.002892,
     "end_time": "2024-11-27T17:57:02.539819",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.536927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following function trains the model using the given hyperparameters and predicts ratings for the test dataset. It iteratively updates the model parameters (latent factors and biases) and saves the predicted ratings to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b530b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T17:57:02.548034Z",
     "iopub.status.busy": "2024-11-27T17:57:02.547612Z",
     "iopub.status.idle": "2024-11-27T17:57:02.562630Z",
     "shell.execute_reply": "2024-11-27T17:57:02.561656Z"
    },
    "papermill": {
     "duration": 0.022291,
     "end_time": "2024-11-27T17:57:02.565243",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.542952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for trainning the model using the provided parameters and predict ratings on the test set\n",
    "def train_and_predict(train_df, test_df, latent_dim, learning_rate, reg_param, n_epochs=20):\n",
    "\n",
    "    print(f\"Training model with params: Latent Dim: {latent_dim}, Learning Rate: {learning_rate}, Regularization: {reg_param}, Epochs: {n_epochs}\")\n",
    "    \n",
    "    # Map user and book IDs to numerical indices\n",
    "    user_to_index = {user_id: idx for idx, user_id in enumerate(train_df['user_id'].unique())}\n",
    "    book_to_index = {book_id: idx for idx, book_id in enumerate(train_df['book_id'].unique())}\n",
    "\n",
    "    # Number of unique users and books\n",
    "    n_users = len(user_to_index)\n",
    "    n_items = len(book_to_index)\n",
    "\n",
    "    # Initialize latent factor matrices for users (P) and books (Q) and biases\n",
    "    P = np.random.normal(scale=0.01, size=(n_users, latent_dim))  # User latent factors\n",
    "    Q = np.random.normal(scale=0.01, size=(n_items, latent_dim))  # Book latent factors\n",
    "    mu = train_df['rating'].mean()  # Global mean rating\n",
    "    b_u = np.zeros(n_users)  # User biases\n",
    "    b_i = np.zeros(n_items)  # Book biases\n",
    "\n",
    "    # Training loop: iterate over epochs to optimize the model\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0  # Track the cumulative loss for the epoch\n",
    "        for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "            user_idx = user_to_index.get(row['user_id'])  # Map user ID to index\n",
    "            book_idx = book_to_index.get(row['book_id'])  # Map book ID to index\n",
    "            rating = row['rating']  # Actual rating\n",
    "\n",
    "            # Skip if user or book is not in the training data\n",
    "            if user_idx is None or book_idx is None:\n",
    "                continue\n",
    "\n",
    "            # Predict the rating\n",
    "            pred_rating = mu + b_u[user_idx] + b_i[book_idx] + np.dot(P[user_idx], Q[book_idx])\n",
    "            error = rating - pred_rating  # Compute the error\n",
    "\n",
    "            # Update biases and latent factors using gradient descent\n",
    "            b_u[user_idx] += learning_rate * (error - reg_param * b_u[user_idx])  # Update user bias\n",
    "            b_i[book_idx] += learning_rate * (error - reg_param * b_i[book_idx])  # Update book bias\n",
    "            P[user_idx] += learning_rate * (error * Q[book_idx] - reg_param * P[user_idx])  # Update user latent factors\n",
    "            Q[book_idx] += learning_rate * (error * P[user_idx] - reg_param * Q[book_idx])  # Update book latent factors\n",
    "\n",
    "            # Accumulate the squared error for loss tracking\n",
    "            total_loss += error**2\n",
    "\n",
    "        # Print the cumulative loss after each epoch\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Generate predictions for the test set\n",
    "    predictions = []\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting\"):\n",
    "        user_idx = user_to_index.get(row['user_id'])  # Map user ID to index\n",
    "        book_idx = book_to_index.get(row['book_id'])  # Map book ID to index\n",
    "\n",
    "        # Predict the rating for user-book pair or fallback to the global mean\n",
    "        if user_idx is not None and book_idx is not None:\n",
    "            pred_rating = mu + b_u[user_idx] + b_i[book_idx] + np.dot(P[user_idx], Q[book_idx])\n",
    "        else:\n",
    "            pred_rating = mu  # Fallback to global mean rating if user or book is not in training data\n",
    "\n",
    "        # Append the prediction to the results\n",
    "        predictions.append({'id': row['id'], 'rating': pred_rating})\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    predictions_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Predictions saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a2c21",
   "metadata": {
    "papermill": {
     "duration": 0.002946,
     "end_time": "2024-11-27T17:57:02.572752",
     "exception": false,
     "start_time": "2024-11-27T17:57:02.569806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we directly use the optimal hyperparameters, that we found in an intermediate computing notebook that calculated them by hyperparameterization tuning with grid search:\n",
    "- Latent dimensions: 75\n",
    "- Learning rate: 0.01\n",
    "- Regularization: 0.1\n",
    "\n",
    "We train for 20 epochs, providing enough iterations to ensure convergence while minimizing the risk of overfitting from excessive training.\n",
    "\n",
    "The model is trained on the training dataset, and predictions for the test dataset are saved in `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8901a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 75  # Optimal number of latent dimensions\n",
    "learning_rate = 0.01  # Optimal learning rate\n",
    "reg_param = 0.1  # Optimal regularization parameter\n",
    "n_epochs = 20  # Number of epochs chosen\n",
    "\n",
    "# Train the model and make predictions\n",
    "train_and_predict(train_df, test_df, latent_dim, learning_rate, reg_param, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9915979,
     "sourceId": 87197,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1041.474879,
   "end_time": "2024-11-27T18:14:19.032435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-27T17:56:57.557556",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
