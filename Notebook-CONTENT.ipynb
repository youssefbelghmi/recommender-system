{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3d5a5d-ccd7-4cf5-bc63-fb0a75078498",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "This project focuses on building a recommender system that predicts user ratings for books based on their past interactions. By understanding user preferences, the system aims to suggest books that align with individual tastes, enhancing the reading experience through personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57866391-8986-4a49-a4e5-1e6ab52cead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f904929-9d06-4a3f-83be-b5eb5cb5e584",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9db56-513c-46d1-a412-0c63fc132cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # To compute cosine similarity between vectors\n",
    "from sentence_transformers import SentenceTransformer  # To generate sentence embeddings for semantic similarity\n",
    "from tqdm import tqdm  # For progress bars \n",
    "\n",
    "import pandas as pd  # For data manipulation and CSV file handling\n",
    "import numpy as np  # For numerical operations \n",
    "\n",
    "import requests  # To make HTTP requests \n",
    "import time  # To introduce delays \n",
    "import glob  # To find and list files using patterns \n",
    "import csv  # For handling CSV file operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd86fd9-ad70-420f-8140-1c739e2ec6e1",
   "metadata": {},
   "source": [
    "Before we start, we need to load the three files needed for the project:\n",
    "- `train.csv`: contains the user ratings for the books.\n",
    "- `test.csv`: contains the user-book pairs for which we need to predict a rating.\n",
    "- `books.csv`: contains information about the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a3401-2cf4-475c-9b8b-2a9c4a5456cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training, test, and books data\n",
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "test_df = pd.read_csv(\"Data/test.csv\")\n",
    "books_df = pd.read_csv(\"Data/books.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b0b49-9730-4a40-b069-0778db232834",
   "metadata": {},
   "source": [
    "## Content-Based Recommendation Method\n",
    "This section is devoted to the implementation of a content-based recommendation method to predict the rating a user might give to a book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484094b-6c46-410a-b9ad-6188425e93f7",
   "metadata": {},
   "source": [
    "First of all, we need to collect metadata (title, authors, themes and description) about the books to establish similarities between them. To do this, we use two APIs: Open Library for themes, titles and authors, and Google Books for detailed descriptions, based on the unique ISBN of the books. This metadata is then concatenated into a content column of a new DataFrame, which will serve as a basis to allow us to compare books and build a content-based recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa312b-dacd-4a7d-a8d8-6b7d49deee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Personal Google API keys (use multiple keys to avoid request limits)\n",
    "API_KEYS = [\n",
    "    \"***************************************\",\n",
    "    \"***************************************\",\n",
    "    \"***************************************\"\n",
    "]\n",
    "api_index = 0  # Start with the first API key\n",
    "\n",
    "# Function to fetch metadata via Open Library API\n",
    "def fetch_metadata_openlibrary(isbn):\n",
    "    url = f\"https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}&jscmd=data&format=json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        key = f\"ISBN:{isbn}\"\n",
    "        \n",
    "        if key in data:\n",
    "            book_data = data[key]\n",
    "            title = book_data.get('title', '')\n",
    "            authors = ', '.join(author['name'] for author in book_data.get('authors', []))\n",
    "            subjects = book_data.get('subjects', [])\n",
    "            themes = ', '.join(\n",
    "                subject['name'] if isinstance(subject, dict) else subject\n",
    "                for subject in subjects\n",
    "            )\n",
    "            return title, authors, themes\n",
    "        return '', '', ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metadata for ISBN {isbn}: {e}\")\n",
    "        return '', '', ''\n",
    "\n",
    "# Function to fetch description via Google Books API\n",
    "def fetch_description_googlebooks(isbn, retries=3, delay=5):\n",
    "    global api_index\n",
    "    for attempt in range(retries):\n",
    "        api_key = API_KEYS[api_index]\n",
    "        url = f\"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}&key={api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limit reached with key {api_key}. Switching API key...\")\n",
    "                api_index = (api_index + 1) % len(API_KEYS)\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if 'items' in data and len(data['items']) > 0:\n",
    "                book_data = data['items'][0]['volumeInfo']\n",
    "                description = book_data.get('description', '')\n",
    "                return description\n",
    "            return ''\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching description for ISBN {isbn} (attempt {attempt + 1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return ''\n",
    "\n",
    "# Define the output file base path\n",
    "base_output_path = \"books_content\"\n",
    "\n",
    "# Iterate over the rows in the DataFrame and create files every 500 books\n",
    "batch_size = 500\n",
    "metadata = []\n",
    "batch_number = 1\n",
    "\n",
    "for i, row in tqdm(enumerate(books_df.itertuples(), start=1), total=len(books_df), desc=\"Fetching metadata\"):\n",
    "    isbn = row.ISBN\n",
    "    title, authors, themes = fetch_metadata_openlibrary(isbn)\n",
    "    description = fetch_description_googlebooks(isbn)\n",
    "    content = (\n",
    "        (title or '') + ' ' +\n",
    "        (authors or '') + ' ' +\n",
    "        (themes or '') + ' ' +\n",
    "        (description or '')\n",
    "    ).strip()\n",
    "    metadata.append({'book_id': row.book_id, 'content': content})\n",
    "    \n",
    "    # Save and reset metadata every 500 books\n",
    "    if i % batch_size == 0 or i == len(books_df):\n",
    "        output_path = f\"{base_output_path}_{batch_number}.csv\"\n",
    "        with open(output_path, mode='w', encoding='utf-8', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['book_id', 'content'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(metadata)\n",
    "        print(f\"Saved {output_path}\")\n",
    "        metadata = []  # Reset metadata for the next batch\n",
    "        batch_number += 1\n",
    "        \n",
    "    # Pause to avoid API limits\n",
    "    time.sleep(2)\n",
    "\n",
    "# Set path for intermediate files\n",
    "file_pattern = \"books_content_*.csv\"\n",
    "\n",
    "# Retrieve files in correct order based on number\n",
    "file_list = sorted(glob.glob(file_pattern), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "# Concatenate files\n",
    "all_books_content_df = pd.concat([pd.read_csv(file) for file in file_list], ignore_index=True)\n",
    "\n",
    "# Save the final file\n",
    "output_path = \"all_books_content.csv\"\n",
    "all_books_content_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08799667-7e3f-494f-b03a-7618dd2f1e18",
   "metadata": {},
   "source": [
    "The rest will be done using the dataframe created previously, so we need to import it. The file all_books_content.csv has two columns, book_id and content, corresponding to the metadata taken from the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc949e4-9152-45a1-9645-aaeab4534863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier all_books_content.csv\n",
    "all_books_content_df = pd.read_csv(\"all_books_content.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea87bb-09ec-4265-a177-ba4979eda8e0",
   "metadata": {},
   "source": [
    "We construct a user-item matrix from the training data. Each row represents a user, and each column represents a book. The cell values correspond to user ratings for books. This matrix will be used to find books that a user has already rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d19c03-494f-479f-99b1-cf942aec806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map user_id and book_id to matrix indices\n",
    "unique_users = sorted(train_df['user_id'].unique())\n",
    "unique_books = sorted(train_df['book_id'].unique())\n",
    "\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "book_to_index = {book_id: idx for idx, book_id in enumerate(unique_books)}\n",
    "\n",
    "# Initialize the user-item matrix with zeros\n",
    "n_users = len(unique_users)\n",
    "n_books = len(unique_books)\n",
    "data_train = np.zeros((n_users, n_books))  # Rows: users, Columns: books\n",
    "\n",
    "# Populate the matrix with user ratings\n",
    "for _, row in train_df.iterrows():\n",
    "    user_idx = user_to_index[row['user_id']]\n",
    "    book_idx = book_to_index[row['book_id']]\n",
    "    data_train[user_idx, book_idx] = row['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187ae61-eea0-4a29-a855-a9de1877ef7e",
   "metadata": {},
   "source": [
    "We can use the `SentenceTransformer` model to convert the textual content of books into dense numerical vectors (embeddings). These embeddings capture the semantic meaning of the book content, which will be used to calculate similarity between books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964938b4-2abf-4c00-a4a8-0abca4f2c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing content with empty strings\n",
    "all_books_content_df['content'] = all_books_content_df['content'].fillna('')\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the book content\n",
    "embeddings = model.encode(all_books_content_df['content'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6aada-a22a-4db4-a5ea-ef585c1a4747",
   "metadata": {},
   "source": [
    "Using the embeddings, we calculate a cosine similarity matrix, where rows and columns represent books and the value at (i, j) represents the similarity between book i and book j. We also handle missing content (NaN) by setting their similarity values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adb3a3-137a-4a07-852e-0add12947867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Handle missing content by setting rows and columns corresponding to NaN to 0\n",
    "nan_indices = all_books_content_df['content'] == ''\n",
    "similarity_matrix[nan_indices, :] = 0\n",
    "similarity_matrix[:, nan_indices] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9e353-afb8-47c5-8a1f-7bf3357aeef9",
   "metadata": {},
   "source": [
    "Let's save the obtained similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf9129-eeaf-4f91-a25c-ff9e497c73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the similarity matrix to a DataFrame\n",
    "similarity_matrix_df = pd.DataFrame(similarity_matrix)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "similarity_matrix_df.to_csv('similarity_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d937d-d766-453b-807a-21f78f623bb3",
   "metadata": {},
   "source": [
    "The next step consists to predict ratings for the test data. For each user-book pair in the test dataset, we have to identify books rated by the user, calculate a weighted average of ratings for similar books based on the similarity scores, and if no similar books are found, fallback to the user's average rating or a random rating between zero and five. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d5d76-d774-445a-b781-902fd9935274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for storing predictions\n",
    "predictions = {}\n",
    "\n",
    "# Predict ratings for each test pair\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting ratings\"):\n",
    "    query_id = row['id']\n",
    "    user_id = row['user_id']\n",
    "    book_id = row['book_id']\n",
    "    \n",
    "    # Skip if user or book is not in the matrix\n",
    "    if user_id not in user_to_index or book_id not in book_to_index:\n",
    "        predictions[query_id] = None\n",
    "        continue\n",
    "    \n",
    "    user_idx = user_to_index[user_id]\n",
    "    book_idx = book_to_index[book_id]\n",
    "    \n",
    "    # Find books rated by the user\n",
    "    user_ratings = data_train[user_idx, :]\n",
    "    rated_items = np.where(user_ratings > 0)[0]\n",
    "    \n",
    "    # Calculate weighted rating\n",
    "    numerator, denominator = 0, 0\n",
    "    for neighbor_idx in rated_items:\n",
    "        similarity = similarity_matrix[book_idx, neighbor_idx]\n",
    "        rating = data_train[user_idx, neighbor_idx]\n",
    "        numerator += similarity * rating\n",
    "        denominator += abs(similarity)\n",
    "    \n",
    "    # Handle cases with no neighbors\n",
    "    if denominator > 0:\n",
    "        predicted_rating = numerator / denominator\n",
    "    else:\n",
    "        if np.any(user_ratings > 0):\n",
    "            predicted_rating = user_ratings[user_ratings > 0].mean()\n",
    "        else:\n",
    "            predicted_rating = np.random.uniform(0.0, 5.0)\n",
    "    \n",
    "    # Store the prediction\n",
    "    predictions[query_id] = predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a451399-4074-4ab8-90ba-d22164504607",
   "metadata": {},
   "source": [
    "Finally, the predicted ratings are saved in a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c8634-31bd-41b3-a2d9-c141827d51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions dictionary to DataFrame\n",
    "predictions_df = pd.DataFrame(list(predictions.items()), columns=['id', 'rating'])\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv(\"content_based_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
